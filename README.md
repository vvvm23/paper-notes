# (Deep Learning) Paper Notes

A collection of cool papers I have read, a summary of them, the key points and
some of my own personal notes.

> If on vim, use `gF` to follow a link in the same split or `gf` to follow in a
> new split.

> Standard format should follow: `<name> [[notes](note path)][[paper](arXiv link)][[other stuff](link)]`

Follow [[this]](read.md) link for the list of papers to read.

## September, 2021

- Structured Denoising Diffusion Models in Discrete State-Spaces
  [[notes](notes/structured-ddm-in-discrete-space.md)][[paper](https://arxiv.org/abs/2107.03006)]

- Score-based Generative Modeling in Latent Space
  [[notes](notes/score-based-in-latent-space.md)][[paper](https://arxiv.org/abs/2106.05931)]

- Escaping the Big Data Paradigm with Compact Transformers
  [[notes](notes/compact-transformer.md)][[paper](https://arxiv.org/abs/2104.05704)][[code](https://github.com/SHI-Labs/Compact-Transformers)][[blog](https://medium.com/pytorch/training-compact-transformers-from-scratch-in-30-minutes-with-pytorch-ff5c21668ed5)]
