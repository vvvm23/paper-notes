# (Deep Learning) Paper Notes

A collection of cool papers I have read, a summary of them, the key points and
some of my own personal notes.

> If on vim, use `gF` to follow a link in the same split or `gf` to follow in a
> new split.

> Standard format should follow: `<name> [[notes](note path)][[paper](arXiv link)][[other stuff](link)]`

Follow [[this]](reading-list.md) link for the list of papers to read.

## October, 2021
- Vector-quantized Image Modeling with Improved VQGAN
  [[notes](notes/improved-vqgan.md)][[paper](https://openreview.net/forum?id=pfNyExj7z2)]

- WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis
  [[notes](notes/wavegrad2.md)][[paper](https://arxiv.org/abs/2106.09660)]

- SoundStream: An End-to-End Neural Audio Codec
  [[notes](notes/soundstream.md)][[paper](https://arxiv.org/abs/2107.03312)]

- Autoregressive Diffusion Models
  [[notes](notes/ardm.md)][[paper](https://arxiv.org/abs/2110.02037)]

- VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models
  [[notes](notes/vaebm.md)][[paper](https://arxiv.org/abs/2010.00654)]

## September, 2021

- Structured Denoising Diffusion Models in Discrete State-Spaces
  [[notes](notes/structured-ddm-in-discrete-space.md)][[paper](https://arxiv.org/abs/2107.03006)]

- Score-based Generative Modeling in Latent Space
  [[notes](notes/score-based-in-latent-space.md)][[paper](https://arxiv.org/abs/2106.05931)]

- Escaping the Big Data Paradigm with Compact Transformers
  [[notes](notes/compact-transformer.md)][[paper](https://arxiv.org/abs/2104.05704)][[code](https://github.com/SHI-Labs/Compact-Transformers)][[blog](https://medium.com/pytorch/training-compact-transformers-from-scratch-in-30-minutes-with-pytorch-ff5c21668ed5)]
